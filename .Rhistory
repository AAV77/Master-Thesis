colnames(d.Inflation) <- gsub("^X", "", colnames(d.Inflation))
d.Inflation <- d.Inflation # %>%
#mutate(`2024` = NA)
# Working with GDP per Capita ->
# Source: https://data.worldbank.org/indicator/NY.GDP.PCAP.CD
path <- "Data/d.GDP/API_NY.GDP.PCAP.CD_DS2_en_csv_v2_3401556.csv"
d.GDP <- read.csv(path, header = T, skip = 4)
# Extracting indicators and putting them in a separate running tally
indicators <- unique_combinations(d.GDP, "Indicator.Name", "Indicator.Code")
colnames(indicators) <- colnames(d.Indicators)
d.Indicators <- rbind(d.Indicators, indicators)
remove(indicators)
# Cleaning Data d.GDP
d.GDP <- d.GDP %>%
filter(Country.Code %in% v.Country_Codes) %>%
select(Country.Code, X2019:X2023)  # Removed Indicator.Code
# Rename first column to "Country"
colnames(d.GDP)[1] <- "Country"
# Remove 'X' prefix from year columns
colnames(d.GDP) <- gsub("^X", "", colnames(d.GDP))
d.GDP <- d.GDP #%>%
#mutate(`2024` = NA)
# Working with Gross Domestic Savings (% of GDP) ->
# Source: https://data.worldbank.org/indicator/NY.GDS.TOTL.ZS
path <- "Data/d.Savings/API_NY.GDS.TOTL.ZS_DS2_en_csv_v2_1988.csv"
d.GDS <- read.csv(path, header = T, skip = 4)
# Extracting indicators and putting them in a separate running tally
indicators <- unique_combinations(d.GDS, "Indicator.Name", "Indicator.Code")
colnames(indicators) <- colnames(d.Indicators)
d.Indicators <- rbind(d.Indicators, indicators)
remove(indicators)
# Cleaning Data d.GDS
d.GDS <- d.GDS %>%
filter(Country.Code %in% v.Country_Codes) %>%
select(Country.Code, X2019:X2023)  # Removed Indicator.Code
# Rename first column to "Country"
colnames(d.GDS)[1] <- "Country"
# Remove 'X' prefix from year columns
colnames(d.GDS) <- gsub("^X", "", colnames(d.GDS))
#d.GDS <- d.GDS #%>%
#mutate(`2024` = NA)
# Working with Personal Remittances Received (% of GDP) ->
# Source: https://data.worldbank.org/indicator/BX.TRF.PWKR.DT.GD.ZS
path <- "Data/d.RemittancesGDP/API_BX.TRF.PWKR.DT.GD.ZS_DS2_en_csv_v2_3426.csv"
d.RR <- read.csv(path, header = T, skip = 4)
# Extracting indicators and putting them in a separate running tally
indicators <- unique_combinations(d.RR, "Indicator.Name", "Indicator.Code")
colnames(indicators) <- colnames(d.Indicators)
d.Indicators <- rbind(d.Indicators, indicators)
remove(indicators)
# Cleaning Data d.RR
d.RR <- d.RR %>%
filter(Country.Code %in% v.Country_Codes) %>%
select(Country.Code, X2019:X2023)  # Removed Indicator.Code
# Rename first column to "Country"
colnames(d.RR)[1] <- "Country"
# Remove 'X' prefix from year columns
colnames(d.RR) <- gsub("^X", "", colnames(d.RR))
d.RR <- d.RR #%>%
#mutate(`2024` = NA)
# Working with External Debt (% of GDP) ->
# Source: https://www.focus-economics.com/economic-indicator/external-debt/
path <- "Data/d.External_Debt_GDP/d.External_Debt_GDP.xlsx"
# Loading Raw Data
d.ED <- read.xlsx(path)
# Adding World Bank Codes
d.ED$Country <- countrycode(d.ED$Country, "country.name", "iso3c")
# warning about unconnected code can be safely ignored since the dataset d.Adoption does not contain the two unmatched countries (Ivory Coast, Kosovo) anyways.
#for some reason, column 2,3 did not save as numeric
d.ED$'2019' <- as.numeric(d.ED$'2019')
d.ED$'2020' <- as.numeric(d.ED$'2020')
d.ED <- d.ED #%>%
#mutate(`2024` = NA)
path <- "Data/d.Adoption/Statista2024.xlsx"
d.Adoption <- read.xlsx(path, sheet = "Data")
# Convert country names to ISO3 codes
d.Adoption$Country <- countrycode(d.Adoption$Country, "country.name", "iso3c")
selected_year <- "2023"  # Changeable variable
# Rename columns for compatibility
d.Adoption_long <- d.Adoption %>%
rename(iso_a3 = Country) %>%
select(iso_a3, all_of(selected_year))
# Load world map data
world <- ne_countries(scale = "medium", returnclass = "sf")
# Merge dataset with world map
world_data <- left_join(world, d.Adoption_long, by = "iso_a3")
# Plot world map
ggplot(data = world_data) +
geom_sf(aes(fill = get(selected_year))) +
scale_fill_gradientn(colors = c("purple", "green", "yellow"), na.value = "gray") +
labs(#title = paste("Adoption Data for Year", selected_year),
fill = "% Adoption") +
theme_minimal()
# Working Bitcoin Adoption->
# Source: https://www.statista.com/statistics/730876/cryptocurrency-maket-value/
path <- "Data/d.Market_Cap_BTC/Statista_Crypto_Market_Cap.xlsx"
d.Market_Cap <- read.xlsx(path,sheet = "Data")
d.Market_Cap$Date <- as.Date(d.Market_Cap$Date, format="%b %d, %Y")
ggplot(d.Market_Cap, aes(x = Date, y = Capitalization)) +
geom_line(color = "blue", size = 1) +   # Line plot
scale_x_date(date_labels = "%b %Y", date_breaks = "3 months",
limits = c(min(d.Market_Cap$Date), max(d.Market_Cap$Date)),
expand = c(0, 0)) +  # Prevents ggplot from adding extra space
labs(
x = "Date",
y = "Market Cap (in Billion U.S. Dollars)") +
theme_minimal() +
theme(
axis.text.x = element_text(angle = 60, hjust = 1, size = 6),  # Adjust x-axis text
panel.grid.major = element_line(size = 0.2, linetype = "solid", color = "gray80"),  # Reduce major gridline thickness
panel.grid.minor = element_blank()  # Completely remove minor gridlines
)
# Remove Taiwan (TWN)
d.Adoption <- d.Adoption %>% filter(Country != "TWN")
world <- ne_countries(scale = "medium", returnclass = "sf")
display_country_codes <- d.Adoption$Country
world <- world %>%
mutate(highlight = ifelse(iso_a3 %in% display_country_codes, "Yes", "No"))
ggplot(data = world) +
geom_sf(aes(fill = highlight), color = "black", size = 0.01) +
scale_fill_manual(values = c("Yes" = "blue", "No" = "lightgray")) +
theme_minimal() +
labs(fill = "Data Availability?")
# Display table
d.Adoption %>%
head() %>%
kable(col.names = c("Country", "2019", "2020", "2021", "2022", "2023", "2024"),
caption = "Statista (2024a) Data: \\ Share (Percentage) of Respondents Who Reported Using Cryptocurrency in Select Years") %>%
kable_styling(full_width = FALSE, latex_options = "hold_position")
# Create the data frame with citations and special characters handled
lit_data_sources <- data.frame(
Indicator = c( "Inflation, consumer prices (annual %)",
"Gross domestic savings (% of GDP)",
"GDP per capita (current US$)",
"Personal remittances, received (% of GDP)",
"External Debt (% of GDP)",
"Political Corruption Index (see below)",
"Bespoke Capital Controls Index (see below)"
#"Mobile Access (%)",
),
Proxy_for = c("Currency Stability",
"Investment",
"Wealth",
"Remittances",
"Risk of Sovereign Default",
"Sins",
"Capital Controls"
#"Technology",
),
Source = c("World Bank (2024c)",
"World Bank (2024b)",
"World Bank (2024a)",
"World Bank (2024d)",
"Focus Economics (2024)",
"V-Dem (2024)",
"IMF (2024)"
#"World Bank",
)
)
# Generate the table using kable
lit_data_sources %>%
kable(col.names = c("Indicator", "Proxy for", "(Primary) Source"), caption = "Overview of Data Sources for Independent Variables (\\#tab:datatable)") %>%
kable_styling(full_width = F, latex_options = "hold_position")
# install.packages("devtools")
# uncomment above lines when running for the first time
devtools::install_github("vdeminstitute/vdemdata")
library(vdemdata)
d.Corruption <- vdem[, c("country_name", "year", "v2x_corr")] # relevant variables
d.Corruption <- d.Corruption %>%
filter(year %in% c(2019: 2024)) # relevant years
d.Corruption$Country <- countrycode(d.Corruption$country_name, "country.name", "iso3c")
# above: world bank / ISO 3 code added
# Renaming, dropping and re-arranging column
d.Corruption <- d.Corruption %>%
rename(Corruption = last_col(offset = 1)) %>%
select(Country, everything(), -country_name)
# Dropping Gaza / Westbank / NA (represents Kosovo, Somaliland and Zanzibar: These are causing aggregation issues and are not in the depdendent variable so can be safely removed
d.Corruption <- d.Corruption %>%
filter(!Country %in% c("PSE", NA))
# Converting to Wide Format Panel Data
d.Corruption <- d.Corruption %>%
pivot_wider(names_from = year, values_from = Corruption)  #%>%
# mutate(`2024` = NA)
path <- "Data/d.CC/IMF (2024).xlsx"
d.CC <- read.xlsx(path)
d.CC %>%
head() %>%
kable(
col.names = c("Year", "IFS Code", "Country",
"Controls Personal \\ Payments",
"Prior \\ Approval",
"Quantitative \\ Limits",
"Indicative Limits / \\ Bona Fide Test",
"Controls on Personal \\ Capital Transactions"),
escape = FALSE,  # Allows LaTeX commands like \\ to be used
caption = "IMF (2024) Capital Controls Dummy Data",
format = "latex"
) %>%
kable_styling(full_width = F, latex_options = c("scale_down","H")) %>%
column_spec(1, width = "0.7cm") %>%  # Custom width for Year
column_spec(2, width = "1.5cm") %>%    # Custom width for IFS Code
column_spec(3, width = "1.7cm") %>%    # Custom width for Country
column_spec(4, width = "2.2cm") %>%    # Custom width for Controls Personal Payments
column_spec(5, width = "2.0cm") %>%  # Custom width for Prior Approval
column_spec(6, width = "2.3cm") %>%  # Custom width for Quantitative Limits
column_spec(7, width = "3.3cm") %>%  # Custom width for Indicative Limits / Bona Fide Test
column_spec(8, width = "3.5cm")      # Custom width for Controls on Personal Capital Transactions
# Rename the last 5 columns to A, B, C, D, E
colnames(d.CC)[(ncol(d.CC)-4):ncol(d.CC)] <- c("A", "B", "C", "D", "E")
# Compute Index dynamically based on available (non-NA) values
d.CC$IndexCC <- rowSums(d.CC[, c("A", "B", "C", "D", "E")] == d.CC[11,4], na.rm = TRUE) /
rowSums(!is.na(d.CC[, c("A", "B", "C", "D", "E")]), na.rm = TRUE)
d.CC$IndexCC[is.nan(d.CC$IndexCC)] <- NA # #converting 0 data point to NA
d.CC$Country <- countrycode(d.CC$Country, "country.name", "iso3c")
d.CC <- na.omit(d.CC, cols = "Country")
# Above Warnings Countries are not found in the Adoption set, can be safely removed
d.CC <- d.CC %>% select(Country, Year, IndexCC) # rearrannging and removing rows
d.CC <- d.CC %>%
pivot_wider(names_from = Year, values_from = IndexCC) %>%  # make wide format
mutate(`2023` = NA)#, `2024` = NA)
kable(head(d.CC), caption = "Head of Table with Capital Controls Index",
row.names = F)
# Extract unique country names from d.Adoption
relevant_countries <- unique(d.Adoption$Country)
# List of original dataframes
IV_data <- list(
d.CC = d.CC,
d.Corruption = d.Corruption,
d.ED = d.ED,
d.GDP = d.GDP,
d.GDS = d.GDS,
d.Inflation = d.Inflation,
d.RR = d.RR
)
# Find missing countries for each dataset
IV_countries <- lapply(IV_data, function(df) unique(df$Country))
missing_countries <- lapply(IV_countries, function(countries) setdiff(relevant_countries, countries))
# Print missing countries for each dataset
for (name in names(missing_countries)) {
cat("\nMissing countries in", name, ":\n")
print(missing_countries[[name]])
}
# Find countries that are missing in **any** dataset
all_missing_countries <- unique(unlist(missing_countries))
cat("\nCountries in d.Adoption but missing in at least one dataset:\n")
print(all_missing_countries)
# Apply filtering and overwrite original dataset names
filtered_datasets <- lapply(IV_data, function(df) {
df[df$Country %in% relevant_countries, , drop = FALSE]
})
# Assign filtered datasets back to their original names in the environment
list2env(filtered_datasets, envir = .GlobalEnv)
d.Adoption <- d.Adoption %>% select(-'2024')
NGA_GDS <- c("2019" = 19.83, "2020" = 27.38, "2021" = 32.73, "2022" = NA, "2023" = NA)#, "2024" = NA)
NGA_row <- which(d.GDS$Country == "NGA")
d.GDS[NGA_row, 2:ncol(d.GDS)] <- NGA_GDS  # Assuming the first column is "Country"
Arg_Inf <- c("2019" = 53.55, "2020" = 42.02, "2021" = 48.41,
"2022" = 72.43, "2023" = 133.49)#, "2024" = NA)
arg_row <- which(d.Inflation$Country == "ARG")
d.Inflation[arg_row, 2:ncol(d.Inflation)] <- Arg_Inf  # Assuming the first column is "Country"
new_countries_External_Debt <- tibble(
Country = c("BEL", "CAN", "FRA", "IRL", "ESP"),
`2019` = c(257.579, 134.234, 235.061, 742.138, 169.725),
`2020` = c(268.416, 149.077, 265.191, 702.699, 200.181),
`2021` = c(260.193, 143.214, 258.592, 677.751, 193.562),
`2022` = c(237.91, 134.904, 244.258, 577.158, 172.805),
`2023` = c(237.68, 143.661, 245.047, 563.897, 165.481)#,
#`2024` = c(NA, NA, NA, NA, NA)
)
# Add only missing countries
existing_countries <- d.ED$Country
missing_countries <- new_countries_External_Debt %>% filter(!Country %in% existing_countries)
# Append missing countries to d.GDS
d.ED <- bind_rows(d.ED, missing_countries)
# Function to run MCAR tests and format results with improved headers
run_mcar_tests <- function(...) {
dfs <- list(...)
df_names <- sapply(substitute(list(...))[-1], deparse)  # Get dataframe names
results <- data.frame(
`Dataset` = character(),
`Test Statistic` = character(),  # More descriptive header
`Degrees of Freedom` = character(),
`P-Value` = character(),  # Improved readability
`Missing Patterns` = character(),
`Missing Percent` = character(),  # More readable title
`Significance Level` = character(),  # Full description
stringsAsFactors = FALSE
)
for (i in seq_along(dfs)) {
df <- dfs[[i]]
df_name <- df_names[i]
# Calculate missing percentage only for numeric fields
numeric_cols <- df[, sapply(df, is.numeric), drop = FALSE]  # Select numeric columns
total_numeric_values <- length(numeric_cols) * nrow(df)  # Total numeric data points
total_missing_values <- sum(is.na(numeric_cols))  # Count missing values in numeric columns
missing_percent <- round((total_missing_values / total_numeric_values) * 100, 2)
# Check for missing values
if (sum(is.na(df)) == 0) {
results <- rbind(results, data.frame(
`Dataset` = df_name,
`Test Statistic` = "-",
`Degrees of Freedom` = "-\u00A0\u00A0\u00A0\u00A0\u00A0\u00A0\u00A0\u00A0\u00A0\u00A0\u00A0\u00A0\u00A0\u00A0\u00A0\u00A0\u00A0No",
`P-Value` = "Missing",
`Missing Patterns` = "Values",
`Missing Percent` = "0.00",  # No missing values
`Significance Level` = "-"
))
} else {
test_result <- naniar::mcar_test(df)
test_result <- as.data.frame(test_result)
p_value <- test_result$p.value[1]
# Determine significance level
significance <- case_when(
p_value < 0.01  ~ "Highly Significant",
p_value < 0.05  ~ "Significant",
TRUE            ~ "Not significant"
)
results <- rbind(results, data.frame(
`Dataset` = df_name,
`Test Statistic` = round(test_result$statistic[1], 2),
`Degrees of Freedom` = test_result$df[1],
`P-Value` = round(p_value, 4),
`Missing Patterns` = test_result$missing.patterns[1],
`Missing Percent` = paste0(missing_percent),
`Significance Level` = significance
))
}
}
# Print results in a clean table using kable
colnames(results) <- c("Dataset", "Test Statistic", "Degrees of Freedom",
"P-Value", "Missing Patterns", "Proportion Missing (%)",
"Significance Level")
kable(results, caption = "MCAR Test Results", digits = 4, format = "markdown")
}
library(dplyr)
library(naniar)
library(knitr)
library(kableExtra)
run_mcar_tests1 <- function(...) {
dfs <- list(...)
df_names <- sapply(substitute(list(...))[-1], deparse)  # Get dataframe names
results <- data.frame(
`Dataset` = character(),
`Test Statistic` = character(),  # More descriptive header
`Degrees of Freedom` = character(),
`P-Value` = character(),  # Improved readability
`Missing Patterns` = character(),
`Missing Percent` = character(),  # More readable title
`Significance Level` = character(),  # Full description
stringsAsFactors = FALSE
)
for (i in seq_along(dfs)) {
df <- dfs[[i]]
df_name <- df_names[i]
# Calculate missing percentage only for numeric fields
numeric_cols <- df[, sapply(df, is.numeric), drop = FALSE]  # Select numeric columns
total_numeric_values <- length(numeric_cols) * nrow(df)  # Total numeric data points
total_missing_values <- sum(is.na(numeric_cols))  # Count missing values in numeric columns
missing_percent <- round((total_missing_values / total_numeric_values) * 100, 2)
# Check for missing values
if (sum(is.na(df)) == 0) {
results <- rbind(results, data.frame(
`Dataset` = df_name,
`Test Statistic` = "-",
`Degrees of Freedom` = "-\u00A0\u00A0\u00A0\u00A0\u00A0\u00A0\u00A0\u00A0\u00A0\u00A0\u00A0\u00A0\u00A0\u00A0\u00A0\u00A0\u00A0No",
`P-Value` = "Missing",
`Missing Patterns` = "Values",
`Missing Percent` = "0.00",  # No missing values
`Significance Level` = "-"
))
} else {
test_result <- naniar::mcar_test(df)
test_result <- as.data.frame(test_result)
p_value <- test_result$p.value[1]
# Determine significance level
significance <- case_when(
p_value < 0.01  ~ "Highly Significant",
p_value < 0.05  ~ "Significant",
TRUE            ~ "Not significant"
)
results <- rbind(results, data.frame(
`Dataset` = df_name,
`Test Statistic` = round(test_result$statistic[1], 2),
`Degrees of Freedom` = test_result$df[1],
`P-Value` = round(p_value, 4),
`Missing Patterns` = test_result$missing.patterns[1],
`Missing Percent` = paste0(missing_percent),
`Significance Level` = significance
))
}
}
# Rename columns for better readability
colnames(results) <- c("Dataset", "Test Statistic", "Degrees of Freedom",
"P-Value", "Missing Patterns", "Proportion Missing (%)",
"Significance Level")
# Print results using kable with column width adjustments
kable(results, caption = "MCAR Test Results", digits = 4, format = "latex", booktabs = TRUE) %>%
kable_styling(full_width = FALSE, latex_options = c("hold_position", "scale_down")) %>%
column_spec(6, width = "4cm") %>%  # Slightly reduce "Proportion Missing (%)"
column_spec(7, width = "5cm")  # Keep "Significance Level" but not too large
}
run_mcar_tests1(d.Adoption, d.Inflation, d.GDS, d.GDP, d.RR, d.ED)
md.pattern(d.CC[,-1])
d.CC[[ncol(d.CC)]] <- d.CC[[ncol(d.CC) - 1]]
remove(filtered_datasets,IV_countries,IV_data,lit_data_sources,world,new_countries_External_Debt)
fill_na_with_nearest_avg <- function(df) {
# Compute column means (excluding 'Country' column), ignoring NA values
col_means <- colMeans(df[, -1], na.rm = TRUE)
for (i in 1:nrow(df)) {
row_values <- df[i, 2:ncol(df)]  # Extract numeric part of the row (excluding country column)
if (all(is.na(row_values))) {
# If all values are NA, replace with column means
df[i, 2:ncol(df)] <- col_means
} else {
for (j in 2:ncol(df)) { # Start from 2nd column to avoid modifying 'Country'
if (is.na(df[i, j])) {
prev_value <- NA
next_value <- NA
# Find previous non-NA value safely
if (j > 2) {
prev_values <- na.omit(as.numeric(df[i, 2:(j-1)]))  # Convert to numeric vector
if (length(prev_values) > 0) {
prev_value <- prev_values[length(prev_values)]  # Last previous value
}
}
# Find next non-NA value safely
if (j < ncol(df)) {
next_values <- na.omit(as.numeric(df[i, (j+1):ncol(df)]))  # Convert to numeric vector
if (length(next_values) > 0) {
next_value <- next_values[1]  # First next value
}
}
# Ensure `prev_value` and `next_value` are single numeric values
prev_exists <- !is.na(prev_value) && is.numeric(prev_value)
next_exists <- !is.na(next_value) && is.numeric(next_value)
# Fill NA using the nearest available values
if (prev_exists && next_exists) {
df[i, j] <- (prev_value + next_value) / 2  # Average of both nearest values
} else if (prev_exists) {
df[i, j] <- prev_value  # Use only the previous value
} else if (next_exists) {
df[i, j] <- next_value  # Use only the next value
}
}
}
}
}
return(df)
}
# Example usage:
# df <- fill_na_with_nearest_avg(df)
# For repeatability, all datasets are still run through the algorithm
d.Adoption_imp <- fill_na_with_nearest_avg(d.Adoption)
d.Inflation_imp <- fill_na_with_nearest_avg(d.Inflation)
d.GDS_imp <- fill_na_with_nearest_avg(d.GDS)
d.GDP_imp <- fill_na_with_nearest_avg(d.GDP)
d.Corruption_imp <- fill_na_with_nearest_avg(d.Corruption)
d.RR_imp<-fill_na_with_nearest_avg(d.RR)
d.CC_imp<-fill_na_with_nearest_avg(d.CC)
d.ED_imp <- fill_na_with_nearest_avg(d.ED)
# Save as _UNimp the unimputed sets
d.Adoption_UNimp <- d.Adoption
d.Inflation_UNimp <- d.Inflation
d.GDS_UNimp <- d.GDS
d.GDP_UNimp <- d.GDP
d.Corruption_UNimp <- d.Corruption
d.RR_UNimp <- d.RR
d.CC_UNimp <- d.CC
d.ED_UNimp <- d.ED
convert_to_long <- function(datasets) {
for (dataset_name in datasets) {
if (exists(dataset_name, envir = .GlobalEnv)) {
dataset <- get(dataset_name, envir = .GlobalEnv)
# Extract the part of the dataset name after "d." and before "_imp"
value_name <- gsub("^d\\.|_imp$", "", dataset_name)
long_name <- paste0(dataset_name, "_long")
long_data <- dataset %>%
pivot_longer(cols = -Country,
names_to = "Year",
values_to = value_name) %>%
mutate(Year = trimws(Year)) %>%  # Remove leading/trailing spaces
mutate(Year = gsub("[^0-9]", "", Year)) %>%  # Remove non-numeric characters
mutate(Year = as.integer(Year))  # Convert cleaned Year to integer
assign(long_name, long_data, envir = .GlobalEnv)
} else {
message(paste("Dataset", dataset_name, "does not exist in the environment."))
}
}
}
dataset_imp_list <- c( "d.CC_imp","d.Corruption_imp","d.ED_imp", "d.GDP_imp", "d.GDS_imp", "d.Inflation_imp","d.RR_imp", "d.Adoption_imp")
convert_to_long(dataset_imp_list)
d.panel <- d.Adoption_imp_long
d.panel<-d.panel %>%
left_join(d.Inflation_imp_long, by= c( "Country","Year")) %>%
left_join(d.GDS_imp_long, by= c( "Country","Year")) %>%
left_join(d.GDP_imp_long, by= c( "Country","Year")) %>%
left_join(d.Corruption_imp_long, by= c( "Country","Year")) %>%
left_join(d.RR_imp_long, by= c( "Country","Year")) %>%
left_join(d.CC_imp_long, by= c( "Country","Year")) %>%
left_join(d.ED_imp_long, by= c( "Country","Year"))
# I performed "Stichproben" for these to check the aggregation is in line with the original data.
set.seed(42)
num_cols <- d.panel[,!names(d.panel) %in% c("Country","Year")] # selecting the numeric columns only
num_cols$CC <- num_cols$CC + rnorm(nrow(num_cols), mean = 0,sd= 0.06) # adding small pertubations
preProcess_model <- preProcess(num_cols, method = "YeoJohnson", center = FALSE, scale = FALSE)
d.panel_YJ <- predict(preProcess_model, num_cols)
d.panel_YJ$Year <-d.panel$Year
d.panel_YJ$Country <-d.panel$Country
#d.panel_YJ$CC <-d.panel$CC
d.panel_YJ <- d.panel_YJ %>%
select(Country,Year, everything())#,CC)
hist(d.panel_YJ$CC)
